{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# dSTORM Localization Filtering Pipeline\n",
    "\n",
    "This pipeline processes ELYRA dSTORM localization data with outline masks and blink-density filtering.\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Load Data** - Read localization files (requires x, y columns)\n",
    "2. **Outline Masking** - Apply periphery/nucleus polygon masks from MATLAB-generated ROI files\n",
    "3. **Blink Density Filtering** - Retain localizations with ≥6 co-localized events within 50nm\n",
    "4. **Output** - Save retained localizations and blinking statistics\n",
    "\n",
    "## Blinking Analysis\n",
    "The pipeline tracks single-blink percentage per cell. Cells with unusually high single-blink fractions may indicate poor labeling or high noise. Outlier detection using IQR and MAD methods flags these cells.\n",
    "\n",
    "## File Naming Convention (matches MATLAB output)\n",
    "- Data file: `{condition}_{time}min_c{cell}.txt`\n",
    "- Periphery outline: `{condition}_{time}min_c{cell}_periphery_outline.txt`\n",
    "- Nucleus outline: `{condition}_{time}min_c{cell}_nucleus_outline.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block0-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 0: Imports and Core Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block0-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "try:\n",
    "    import locan as lc\n",
    "except Exception as e:\n",
    "    lc = None\n",
    "    print(\"WARNING: locan not available, will use pandas fallback.\\n\", e)\n",
    "\n",
    "# Pattern matches: control_5min_c14.txt, iso_20min_c3.txt, etc.\n",
    "RAW_RE = re.compile(r'^(iso|control)_(\\d+)\\s*min_c(\\d+)\\.txt$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "class ParticleDataManager:\n",
    "    \"\"\"Scans directory for data files matching {condition}_{time}min_c{cell}.txt\"\"\"\n",
    "\n",
    "    def __init__(self, base_path=\".\", recursive=False):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.recursive = bool(recursive)\n",
    "        self.file_catalog = {}\n",
    "        self.records = {}\n",
    "        self.scan_for_files()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_cell_number(filepath):\n",
    "        m = re.search(r'c\\s*(\\d+)', filepath.stem, flags=re.IGNORECASE)\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    @staticmethod\n",
    "    def _group_key(cond, minutes):\n",
    "        return f\"{cond.lower()}_{int(minutes)}min\"\n",
    "\n",
    "    def scan_for_files(self):\n",
    "        if not self.base_path.exists():\n",
    "            print(f\"Warning: directory '{self.base_path}' does not exist\")\n",
    "            return\n",
    "        entries = list(self.base_path.rglob(\"*.txt\")) if self.recursive else list(self.base_path.glob(\"*.txt\"))\n",
    "        n_data = 0\n",
    "        for p in entries:\n",
    "            m_raw = RAW_RE.match(p.name)\n",
    "            if not m_raw:\n",
    "                continue\n",
    "            cond, minutes, cell = m_raw.group(1).lower(), int(m_raw.group(2)), int(m_raw.group(3))\n",
    "            self.records[(cond, minutes, cell)] = p\n",
    "            n_data += 1\n",
    "        catalog = {}\n",
    "        for (cond, minutes, cell), path in sorted(self.records.items()):\n",
    "            gkey = self._group_key(cond, minutes)\n",
    "            catalog.setdefault(gkey, []).append(path)\n",
    "        for gkey, files in catalog.items():\n",
    "            catalog[gkey] = sorted(files, key=lambda p: (self.extract_cell_number(p) or 0, p.name))\n",
    "        self.file_catalog = catalog\n",
    "        if self.file_catalog:\n",
    "            print(f\"Scanned '{self.base_path}': {n_data} data files found. Groups:\")\n",
    "            for key, files in sorted(self.file_catalog.items()):\n",
    "                print(f\"  - {key}: {len(files)} cells\")\n",
    "        else:\n",
    "            print(f\"No data files found in '{self.base_path}'\")\n",
    "\n",
    "    def get_available_cells(self, condition, time):\n",
    "        condition = str(condition).strip().lower()\n",
    "        minutes = int(re.search(r\"(\\d+)\", str(time)).group(1))\n",
    "        out = {}\n",
    "        for (cond, mins, cell), path in self.records.items():\n",
    "            if cond == condition and mins == minutes:\n",
    "                out[cell] = path\n",
    "        return dict(sorted(out.items()))\n",
    "\n",
    "    def list_available_files(self):\n",
    "        if not self.file_catalog:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        for key, files in sorted(self.file_catalog.items()):\n",
    "            print(f\"\\n{key.upper()}:\")\n",
    "            print(\"-\" * 40)\n",
    "            for fp in files:\n",
    "                cnum = self.extract_cell_number(fp)\n",
    "                print(f\"  Cell {cnum}: {fp.name}\")\n",
    "\n",
    "\n",
    "class ParticleDataLoader:\n",
    "    \"\"\"Loads localization files with automatic column mapping.\"\"\"\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = Path(filepath)\n",
    "        self.locdata = None\n",
    "        self.df = None\n",
    "        self.column_mapping = {}\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        try:\n",
    "            if lc is not None:\n",
    "                self.locdata = lc.load_Elyra_file(str(self.filepath))\n",
    "                self.df = self.locdata.dataframe\n",
    "            else:\n",
    "                raise ImportError(\"locan not available\")\n",
    "        except Exception:\n",
    "            # Fallback to pandas\n",
    "            try:\n",
    "                self.df = pd.read_csv(self.filepath, sep='\\t', encoding='latin1')\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {self.filepath}: {e}\")\n",
    "                self.df = pd.DataFrame()\n",
    "                return\n",
    "        self.map_columns()\n",
    "        print(f\"Loaded {len(self.df)} localizations from {self.filepath.name}\")\n",
    "    \n",
    "    def map_columns(self):\n",
    "        if self.df is None or self.df.empty:\n",
    "            return\n",
    "        column_variations = {\n",
    "            'x': ['Position X [nm]', 'position_x', 'x', 'X', 'x_nm'],\n",
    "            'y': ['Position Y [nm]', 'position_y', 'y', 'Y', 'y_nm'],\n",
    "            'photons': ['Number Photons', 'photons', 'n_photons', 'intensity', 'Photons'],\n",
    "            'precision': ['Precision [nm]', 'precision', 'uncertainty', 'Precision'],\n",
    "            'frame': ['First Frame', 'frame', 'Frame', 'first_frame'],\n",
    "            'background': ['Background variance', 'background', 'Background']\n",
    "        }\n",
    "        actual_columns = self.df.columns.tolist()\n",
    "        for key, variations in column_variations.items():\n",
    "            for var in variations:\n",
    "                if var in actual_columns:\n",
    "                    self.column_mapping[key] = var\n",
    "                    break\n",
    "                for col in actual_columns:\n",
    "                    if col.lower() == var.lower():\n",
    "                        self.column_mapping[key] = col\n",
    "                        break\n",
    "        # Fallback to locan coordinate keys\n",
    "        if ('x' not in self.column_mapping or 'y' not in self.column_mapping) and self.locdata is not None:\n",
    "            if hasattr(self.locdata, 'coordinate_keys') and len(self.locdata.coordinate_keys) >= 2:\n",
    "                self.column_mapping['x'] = self.locdata.coordinate_keys[0]\n",
    "                self.column_mapping['y'] = self.locdata.coordinate_keys[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block1-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 1: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_xy_columns(loader):\n",
    "    \"\"\"Create DataFrame with standardized x,y columns.\"\"\"\n",
    "    df = loader.df.copy()\n",
    "    x_col = loader.column_mapping.get('x')\n",
    "    y_col = loader.column_mapping.get('y')\n",
    "    if x_col is None or y_col is None:\n",
    "        raise KeyError(\"x and y columns not found in data\")\n",
    "    df['x'] = df[x_col].astype(float)\n",
    "    df['y'] = df[y_col].astype(float)\n",
    "    df = df[np.isfinite(df['x']) & np.isfinite(df['y'])]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _retained_canonical_df(loader, df, final_mask):\n",
    "    \"\"\"Build output DataFrame with available columns.\"\"\"\n",
    "    retained = df.loc[final_mask, ['x', 'y']].copy()\n",
    "    for k in ['photons', 'precision', 'frame', 'background']:\n",
    "        if k in loader.column_mapping:\n",
    "            col_name = loader.column_mapping[k]\n",
    "            if col_name in loader.df.columns:\n",
    "                retained[k] = loader.df.loc[retained.index, col_name].values\n",
    "    return retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block2-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 2: Blink Density Calculation\n",
    "\n",
    "Computes the number of localizations within 50nm radius for each point using a KD-tree. This \"blinks per site\" metric identifies clustered multi-blink events vs isolated single-blink noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_blinks_per_site(df, radius_nm=50):\n",
    "    \"\"\"Count localizations within radius_nm for each point.\"\"\"\n",
    "    if 'blinks_per_site' in df.columns:\n",
    "        return df\n",
    "    pts = df[['x', 'y']].to_numpy()\n",
    "    if pts.size == 0:\n",
    "        df['blinks_per_site'] = 0\n",
    "        return df\n",
    "    tree = cKDTree(pts)\n",
    "    neigh = tree.query_ball_tree(tree, r=radius_nm)\n",
    "    df['blinks_per_site'] = np.fromiter((len(n) for n in neigh), dtype=int, count=len(pts))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block3-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 3: Outline File Parsing and Masking\n",
    "\n",
    "Reads periphery and nucleus outline polygon files (from MATLAB) and applies spatial masks:\n",
    "- Points outside the periphery polygon → removed\n",
    "- Points inside the nucleus polygon → removed (if file exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_PERI_END = re.compile(r'(?i)_periphery_outline(?:_v\\d+)?\\.txt$')\n",
    "PAT_NUCL_END = re.compile(r'(?i)_nucleus_outline(?:_v\\d+)?\\.txt$')\n",
    "\n",
    "\n",
    "def _read_outline_points(path):\n",
    "    \"\"\"Read polygon vertices from outline file.\"\"\"\n",
    "    if path is None or not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", comment=\"#\")\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        xcol, ycol = cols.get(\"x\"), cols.get(\"y\")\n",
    "        if xcol is None or ycol is None:\n",
    "            if len(df.columns) >= 2:\n",
    "                xcol, ycol = df.columns[:2]\n",
    "            else:\n",
    "                return None\n",
    "        pts = df[[xcol, ycol]].astype(float).to_numpy()\n",
    "        return pts if len(pts) >= 3 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _polygon_area_nm2(pts):\n",
    "    \"\"\"Calculate polygon area using shoelace formula.\"\"\"\n",
    "    if pts is None or len(pts) < 3:\n",
    "        return 0.0\n",
    "    x, y = pts[:, 0], pts[:, 1]\n",
    "    return 0.5 * abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n",
    "\n",
    "\n",
    "def _find_outline_file(search_dir, stem, kind):\n",
    "    \"\"\"Find matching outline file for a data file.\"\"\"\n",
    "    assert kind in (\"periphery\", \"nucleus\")\n",
    "    pattern = PAT_PERI_END if kind == \"periphery\" else PAT_NUCL_END\n",
    "    \n",
    "    # Try exact match first: stem_periphery_outline.txt\n",
    "    exact = search_dir / f\"{stem}_{kind}_outline.txt\"\n",
    "    if exact.exists():\n",
    "        return exact\n",
    "    \n",
    "    # Search for matching files\n",
    "    for p in search_dir.glob(f\"*{kind}_outline*.txt\"):\n",
    "        if pattern.search(p.name) and p.stem.startswith(stem.split('_')[0]):\n",
    "            # Check if cell number matches\n",
    "            m_data = re.search(r'c(\\d+)', stem, re.IGNORECASE)\n",
    "            m_file = re.search(r'c(\\d+)', p.stem, re.IGNORECASE)\n",
    "            if m_data and m_file and m_data.group(1) == m_file.group(1):\n",
    "                return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def apply_outline_masks(df, stem, search_dir):\n",
    "    \"\"\"Apply periphery and nucleus outline masks.\"\"\"\n",
    "    search_dir = Path(search_dir)\n",
    "    keep = np.ones(len(df), dtype=bool)\n",
    "    info = {\n",
    "        \"periphery_file\": \"\", \"nucleus_file\": \"\",\n",
    "        \"periphery_vertices\": 0, \"nucleus_vertices\": 0,\n",
    "        \"periphery_area_um2\": 0.0, \"nucleus_area_um2\": 0.0, \"final_area_um2\": 0.0,\n",
    "        \"n_removed_outside_periphery\": 0, \"n_removed_in_nucleus\": 0, \"n_kept\": len(df)\n",
    "    }\n",
    "    \n",
    "    pts = df[[\"x\", \"y\"]].to_numpy(np.float64)\n",
    "    \n",
    "    # Periphery mask\n",
    "    peri_file = _find_outline_file(search_dir, stem, \"periphery\")\n",
    "    peri_pts = _read_outline_points(peri_file)\n",
    "    if peri_pts is not None:\n",
    "        info[\"periphery_file\"] = peri_file.name\n",
    "        info[\"periphery_vertices\"] = len(peri_pts)\n",
    "        info[\"periphery_area_um2\"] = _polygon_area_nm2(peri_pts) / 1e6\n",
    "        inside_peri = mpath.Path(peri_pts).contains_points(pts)\n",
    "        info[\"n_removed_outside_periphery\"] = int((~inside_peri).sum())\n",
    "        keep &= inside_peri\n",
    "    \n",
    "    # Nucleus mask\n",
    "    nucl_file = _find_outline_file(search_dir, stem, \"nucleus\")\n",
    "    nucl_pts = _read_outline_points(nucl_file)\n",
    "    if nucl_pts is not None:\n",
    "        info[\"nucleus_file\"] = nucl_file.name\n",
    "        info[\"nucleus_vertices\"] = len(nucl_pts)\n",
    "        info[\"nucleus_area_um2\"] = _polygon_area_nm2(nucl_pts) / 1e6\n",
    "        inside_nucl = mpath.Path(nucl_pts).contains_points(pts)\n",
    "        info[\"n_removed_in_nucleus\"] = int(inside_nucl.sum())\n",
    "        keep &= ~inside_nucl\n",
    "    \n",
    "    info[\"final_area_um2\"] = max(info[\"periphery_area_um2\"] - info[\"nucleus_area_um2\"], 0.0)\n",
    "    info[\"n_kept\"] = int(keep.sum())\n",
    "    \n",
    "    return keep, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block4-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 4: Single-Cell Filter Function\n",
    "\n",
    "Combines outline masking and blink density filtering for a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cell(loader, stem=None, blinks_threshold=5, blinks_radius_nm=50, verbose=True):\n",
    "    \"\"\"Apply outline masks and blink filtering to a single cell.\"\"\"\n",
    "    if stem is None:\n",
    "        stem = Path(loader.filepath).stem\n",
    "    \n",
    "    data_dir = Path(loader.filepath).parent\n",
    "    df = ensure_xy_columns(loader)\n",
    "    n_loaded = len(df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[{stem}] Loaded {n_loaded} points\")\n",
    "    \n",
    "    # Apply outline masks\n",
    "    outline_mask, outline_info = apply_outline_masks(df, stem, data_dir)\n",
    "    if verbose:\n",
    "        peri_status = \"FOUND\" if outline_info[\"periphery_file\"] else \"missing\"\n",
    "        nucl_status = \"FOUND\" if outline_info[\"nucleus_file\"] else \"missing\"\n",
    "        print(f\"[{stem}] Outlines: periphery={peri_status}, nucleus={nucl_status}\")\n",
    "        print(f\"[{stem}] Removed: {outline_info['n_removed_outside_periphery']} outside periphery, \"\n",
    "              f\"{outline_info['n_removed_in_nucleus']} in nucleus\")\n",
    "    \n",
    "    # Filter to outline-passing points\n",
    "    df = df.loc[outline_mask].copy()\n",
    "    \n",
    "    # Compute blink density\n",
    "    df = compute_blinks_per_site(df, radius_nm=blinks_radius_nm)\n",
    "    n_after_outline = len(df)\n",
    "    n_single = int((df[\"blinks_per_site\"] == 1).sum())\n",
    "    pct_single = 100.0 * n_single / n_after_outline if n_after_outline > 0 else 0.0\n",
    "    \n",
    "    # Apply blink threshold\n",
    "    blink_mask = df[\"blinks_per_site\"] >= blinks_threshold\n",
    "    n_final = int(blink_mask.sum())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[{stem}] After outlines: {n_after_outline} points, single-blink: {n_single} ({pct_single:.1f}%)\")\n",
    "        print(f\"[{stem}] Blink filter (>={blinks_threshold}): {n_final} retained\")\n",
    "    \n",
    "    summary = {\n",
    "        \"stem\": stem,\n",
    "        \"n_loaded\": n_loaded,\n",
    "        \"n_after_outline\": n_after_outline,\n",
    "        \"n_removed_outside_periphery\": outline_info[\"n_removed_outside_periphery\"],\n",
    "        \"n_removed_in_nucleus\": outline_info[\"n_removed_in_nucleus\"],\n",
    "        \"periphery_file\": outline_info[\"periphery_file\"],\n",
    "        \"nucleus_file\": outline_info[\"nucleus_file\"],\n",
    "        \"periphery_area_um2\": outline_info[\"periphery_area_um2\"],\n",
    "        \"nucleus_area_um2\": outline_info[\"nucleus_area_um2\"],\n",
    "        \"final_area_um2\": outline_info[\"final_area_um2\"],\n",
    "        \"n_singleblink\": n_single,\n",
    "        \"pct_singleblink\": pct_single,\n",
    "        \"n_final\": n_final,\n",
    "    }\n",
    "    \n",
    "    return df, blink_mask, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block5-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 5: Plotting and Report Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtered_cell(df, keep_mask, title=None, save_path=None, show=True):\n",
    "    \"\"\"Plot retained vs removed localizations.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    x, y = df['x'].to_numpy(), df['y'].to_numpy()\n",
    "    keep = np.asarray(keep_mask, dtype=bool)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 7), dpi=150)\n",
    "    if (~keep).any():\n",
    "        ax.scatter(x[~keep], y[~keep], s=1.5, alpha=0.25, c=\"gray\", label=\"Removed\")\n",
    "    if keep.any():\n",
    "        ax.scatter(x[keep], y[keep], s=2.0, alpha=0.9, c=\"tab:blue\", label=\"Retained\")\n",
    "    \n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"X (nm)\")\n",
    "    ax.set_ylabel(\"Y (nm)\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend(loc=\"upper right\", frameon=False)\n",
    "    \n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def write_filter_report(summary, out_dir, retained_count):\n",
    "    \"\"\"Write filter report matching MATLAB _areas.txt format.\"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    stem = summary[\"stem\"]\n",
    "    \n",
    "    lines = [\n",
    "        f\"File: {stem}.txt\",\n",
    "        f\"Periphery area (um^2): {summary['periphery_area_um2']:.6f}\",\n",
    "        f\"Core/Nucleus area (um^2): {summary['nucleus_area_um2']:.6f}\",\n",
    "        f\"Final area (um^2): {summary['final_area_um2']:.6f}\",\n",
    "        f\"Original points: {summary['n_loaded']}\",\n",
    "        f\"Final points: {retained_count}\",\n",
    "        f\"Total removed: {summary['n_loaded'] - retained_count}\",\n",
    "        f\"Periphery outline file: {summary['periphery_file'] or '(none)'}\",\n",
    "        f\"Nucleus outline file: {summary['nucleus_file'] or '(none)'}\",\n",
    "        f\"Single-blink percentage: {summary['pct_singleblink']:.2f}%\",\n",
    "    ]\n",
    "    \n",
    "    report_path = out_dir / f\"{stem}_filter_report.txt\"\n",
    "    report_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return report_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block6-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 6: Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_cells(pdm, out_dir=None, blinks_threshold=5, blinks_radius_nm=50,\n",
    "                      require_periphery=True, save_png=False):\n",
    "    \"\"\"Process all cells and save filtered outputs.\"\"\"\n",
    "    out_dir = Path(out_dir if out_dir else \"retained_filtered\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BATCH PROCESSING\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Output: {out_dir.resolve()}\")\n",
    "    print(f\"Blink threshold: >= {blinks_threshold} (within {blinks_radius_nm}nm)\")\n",
    "    print(f\"Require periphery outline: {require_periphery}\\n\")\n",
    "    \n",
    "    summaries = []\n",
    "    n_processed, n_skipped = 0, 0\n",
    "    \n",
    "    for key in pdm.file_catalog:\n",
    "        condition, time_str = key.split(\"_\", 1)\n",
    "        available = pdm.get_available_cells(condition, time_str)\n",
    "        \n",
    "        for cell_num, filepath in sorted(available.items()):\n",
    "            stem = filepath.stem\n",
    "            search_dir = filepath.parent\n",
    "            \n",
    "            # Check for periphery outline if required\n",
    "            if require_periphery:\n",
    "                peri_file = _find_outline_file(search_dir, stem, \"periphery\")\n",
    "                if peri_file is None:\n",
    "                    print(f\"[SKIP] {stem}: periphery outline missing\")\n",
    "                    n_skipped += 1\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\n[{stem}] Processing...\")\n",
    "                loader = ParticleDataLoader(filepath)\n",
    "                \n",
    "                df, blink_mask, summary = filter_cell(\n",
    "                    loader, stem=stem,\n",
    "                    blinks_threshold=blinks_threshold,\n",
    "                    blinks_radius_nm=blinks_radius_nm,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Add metadata\n",
    "                summary[\"condition\"] = condition\n",
    "                summary[\"time\"] = time_str\n",
    "                summary[\"cell_num\"] = cell_num\n",
    "                summaries.append(summary)\n",
    "                \n",
    "                # Save retained points\n",
    "                retained = _retained_canonical_df(loader, df, blink_mask)\n",
    "                out_txt = out_dir / f\"{stem}_retained.txt\"\n",
    "                retained.to_csv(out_txt, sep=\"\\t\", index=False)\n",
    "                print(f\"[{stem}] Saved: {out_txt.name} ({len(retained)} rows)\")\n",
    "                \n",
    "                # Save report\n",
    "                write_filter_report(summary, out_dir, len(retained))\n",
    "                \n",
    "                # Save PNG if requested\n",
    "                if save_png:\n",
    "                    out_png = out_dir / f\"{stem}_filtered.png\"\n",
    "                    title = f\"{stem}: {len(retained)}/{summary['n_after_outline']} retained\"\n",
    "                    plot_filtered_cell(df, blink_mask, title=title, save_path=out_png, show=False)\n",
    "                \n",
    "                n_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {stem}: {e}\")\n",
    "    \n",
    "    # Save batch summary\n",
    "    summary_df = pd.DataFrame(summaries) if summaries else pd.DataFrame()\n",
    "    if not summary_df.empty:\n",
    "        summary_df.to_csv(out_dir / \"batch_summary.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"COMPLETE: {n_processed} processed, {n_skipped} skipped\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block7-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 7: Blinking Analysis and Outlier Detection\n",
    "\n",
    "Identifies cells with unusually high single-blink percentages using:\n",
    "- **IQR method**: Points outside Q1 - 1.5×IQR or Q3 + 1.5×IQR\n",
    "- **MAD method**: Modified z-score > 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block7-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_blinking_outliers(summary_df, iqr_k=1.5, mad_z=3.5):\n",
    "    \"\"\"Detect outlier cells based on single-blink percentage.\"\"\"\n",
    "    if summary_df is None or len(summary_df) == 0:\n",
    "        print(\"No data to analyze.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if \"pct_singleblink\" not in summary_df.columns:\n",
    "        print(\"Missing pct_singleblink column.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = summary_df.copy()\n",
    "    df[\"pct\"] = pd.to_numeric(df[\"pct_singleblink\"], errors=\"coerce\")\n",
    "    \n",
    "    if \"condition\" in df.columns:\n",
    "        groups = df.groupby(\"condition\")[\"pct\"]\n",
    "    else:\n",
    "        df[\"_all\"] = \"all\"\n",
    "        groups = df.groupby(\"_all\")[\"pct\"]\n",
    "    \n",
    "    # IQR method\n",
    "    q1 = groups.transform(lambda x: x.quantile(0.25))\n",
    "    q3 = groups.transform(lambda x: x.quantile(0.75))\n",
    "    iqr = q3 - q1\n",
    "    df[\"outlier_iqr\"] = (df[\"pct\"] < q1 - iqr_k * iqr) | (df[\"pct\"] > q3 + iqr_k * iqr)\n",
    "    \n",
    "    # MAD method\n",
    "    median = groups.transform(\"median\")\n",
    "    mad = groups.transform(lambda x: np.median(np.abs(x - np.median(x))))\n",
    "    df[\"z_mod\"] = 0.6745 * (df[\"pct\"] - median) / mad.replace(0, np.nan)\n",
    "    df[\"outlier_mad\"] = df[\"z_mod\"].abs() > mad_z\n",
    "    \n",
    "    df[\"outlier_confirmed\"] = df[\"outlier_iqr\"] & df[\"outlier_mad\"]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nBlinking Outlier Analysis\")\n",
    "    print(f\"  IQR method (k={iqr_k}): {df['outlier_iqr'].sum()} flagged\")\n",
    "    print(f\"  MAD method (z>{mad_z}): {df['outlier_mad'].sum()} flagged\")\n",
    "    print(f\"  Confirmed (both): {df['outlier_confirmed'].sum()} flagged\")\n",
    "    \n",
    "    return df[df[\"outlier_confirmed\"]]\n",
    "\n",
    "\n",
    "def show_blinking_summary(summary_df):\n",
    "    \"\"\"Display single-blink statistics.\"\"\"\n",
    "    if summary_df is None or len(summary_df) == 0:\n",
    "        print(\"No data.\")\n",
    "        return\n",
    "    \n",
    "    cols = [\"condition\", \"time\", \"cell_num\", \"pct_singleblink\", \"n_singleblink\", \"n_after_outline\", \"n_final\"]\n",
    "    cols = [c for c in cols if c in summary_df.columns]\n",
    "    \n",
    "    out = summary_df[cols].copy()\n",
    "    if \"pct_singleblink\" in out.columns:\n",
    "        out[\"pct_singleblink\"] = out[\"pct_singleblink\"].round(2)\n",
    "    \n",
    "    sort_cols = [c for c in [\"condition\", \"time\", \"cell_num\"] if c in out.columns]\n",
    "    if sort_cols:\n",
    "        out = out.sort_values(sort_cols)\n",
    "    \n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(out)\n",
    "    except Exception:\n",
    "        print(out.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data directory\n",
    "BASE_DIR = Path(r\"C:\\Users\\venturasubirav2\\dSTORM\\ALL_DATA\")\n",
    "OUT_DIR = BASE_DIR / \"retained_filtered\"\n",
    "\n",
    "print(f\"Input: {BASE_DIR}\")\n",
    "print(f\"Output: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scan-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for data files\n",
    "pdm = ParticleDataManager(base_path=BASE_DIR, recursive=False)\n",
    "pdm.list_available_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch processing\n",
    "summary_df = process_all_cells(\n",
    "    pdm=pdm,\n",
    "    out_dir=OUT_DIR,\n",
    "    blinks_threshold=6,        # Keep localizations with >= 5 blinks within 50nm\n",
    "    blinks_radius_nm=50,\n",
    "    require_periphery=True,    # Skip cells without periphery outline\n",
    "    save_png=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results: Blinking Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_blinking_summary(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outliers-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = analyze_blinking_outliers(summary_df)\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nConfirmed outlier cells:\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(outliers[[\"stem\", \"pct_singleblink\", \"n_final\"]])\n",
    "    except Exception:\n",
    "        print(outliers[[\"stem\", \"pct_singleblink\", \"n_final\"]].to_string())\n",
    "else:\n",
    "    print(\"\\nNo confirmed outliers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (foscat_env)",
   "language": "python",
   "name": "foscat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
